{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-02-14T21:06:44.269018Z",
     "iopub.status.busy": "2021-02-14T21:06:44.265925Z",
     "iopub.status.idle": "2021-02-14T21:06:45.513395Z",
     "shell.execute_reply": "2021-02-14T21:06:45.514060Z"
    },
    "papermill": {
     "duration": 1.276069,
     "end_time": "2021-02-14T21:06:45.514447",
     "exception": false,
     "start_time": "2021-02-14T21:06:44.238378",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "import os\n",
    "# os.environ[\"OPENBLAS_NUM_THREADS\"] = \"1\"\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.025392,
     "end_time": "2021-02-14T21:06:45.566010",
     "exception": false,
     "start_time": "2021-02-14T21:06:45.540618",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-14T21:06:45.621198Z",
     "iopub.status.busy": "2021-02-14T21:06:45.620321Z",
     "iopub.status.idle": "2021-02-14T21:06:45.690200Z",
     "shell.execute_reply": "2021-02-14T21:06:45.690898Z"
    },
    "papermill": {
     "duration": 0.0994,
     "end_time": "2021-02-14T21:06:45.691143",
     "exception": false,
     "start_time": "2021-02-14T21:06:45.591743",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 10) (10000,)\n",
      "(8000, 10) (2000, 10)\n",
      "(8000,) (2000,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = datasets.make_classification(n_samples=10000, n_features=10)\n",
    "print(X.shape, y.shape)\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state = 2020)\n",
    "print(X_train.shape, X_test.shape)\n",
    "print(y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.026821,
     "end_time": "2021-02-14T21:06:45.745300",
     "exception": false,
     "start_time": "2021-02-14T21:06:45.718479",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-14T21:06:45.802696Z",
     "iopub.status.busy": "2021-02-14T21:06:45.801854Z",
     "iopub.status.idle": "2021-02-14T21:06:46.016244Z",
     "shell.execute_reply": "2021-02-14T21:06:46.015360Z"
    },
    "papermill": {
     "duration": 0.244238,
     "end_time": "2021-02-14T21:06:46.016418",
     "exception": false,
     "start_time": "2021-02-14T21:06:45.772180",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf_dict = {\n",
    "    \n",
    "    \"NB\": {'clf': GaussianNB(),'params': {}},\n",
    "                    \n",
    "    \"LR\": {'clf': LogisticRegression(),\n",
    "           'params' : {'penalty': ['l1','l2', 'elasticnet'],\n",
    "                       'C': [0.001,0.01,0.1,1,10,100,1000]}},\n",
    "    \"SVC\": {'clf': LinearSVC(),\n",
    "            'params': { 'penalty': ['l1','l2'],\n",
    "                       'C': [0.01, 0.1,1,10, 100],}}, \n",
    "\n",
    "    \"RF\": {'clf': RandomForestClassifier(),\n",
    "           'params': {}},\n",
    "    \n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-14T21:06:46.081526Z",
     "iopub.status.busy": "2021-02-14T21:06:46.080836Z",
     "iopub.status.idle": "2021-02-14T21:06:46.084218Z",
     "shell.execute_reply": "2021-02-14T21:06:46.084674Z"
    },
    "papermill": {
     "duration": 0.04143,
     "end_time": "2021-02-14T21:06:46.084848",
     "exception": false,
     "start_time": "2021-02-14T21:06:46.043418",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV \n",
    "from sklearn.metrics import accuracy_score\n",
    "import joblib\n",
    "\n",
    "def classify_data(X_train, y_train, X_test, y_test, verbose = True):\n",
    "    \n",
    "    results_df = pd.DataFrame(data=np.zeros(shape=(len(clf_dict), 4)),\n",
    "                              columns = ['classifier', 'training_time',\n",
    "                                         'train_acc', 'test_acc'])\n",
    "    \n",
    "    for i, key in enumerate(clf_dict.keys()):\n",
    "        start = time.time()\n",
    "        cv = GridSearchCV(clf_dict[key]['clf'],\n",
    "                          clf_dict[key]['params'],\n",
    "                          cv = 5, refit  = True,\n",
    "                          scoring = 'accuracy',\n",
    "                          n_jobs = -1)\n",
    "        \n",
    "        estimator = cv.fit(X_train, y_train)\n",
    "        clf_dict[key]['best_params'] = cv.best_params_\n",
    "        end = time.time()\n",
    "        tdiff = end - start\n",
    "        joblib.dump(cv.best_estimator_, f\"{key}.joblib\")\n",
    "        if verbose:\n",
    "            print(\"{c} training time: {f:.2f} s\".format(c=key,f=tdiff))\n",
    "\n",
    "        train_score = estimator.score(X_train,y_train)\n",
    "        test_score = estimator.score(X_test,y_test)\n",
    "        results_df.loc[i,'classifier'] = key\n",
    "        results_df.loc[i,'train_acc'] = train_score\n",
    "        results_df.loc[i,'test_acc'] = test_score\n",
    "        results_df.loc[i,'training_time'] = tdiff\n",
    "        \n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-14T21:06:46.124209Z",
     "iopub.status.busy": "2021-02-14T21:06:46.123591Z",
     "iopub.status.idle": "2021-02-14T21:06:58.799012Z",
     "shell.execute_reply": "2021-02-14T21:06:58.798373Z"
    },
    "papermill": {
     "duration": 12.696735,
     "end_time": "2021-02-14T21:06:58.799195",
     "exception": false,
     "start_time": "2021-02-14T21:06:46.102460",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB training time: 1.73 s\n",
      "LR training time: 0.40 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/svm/_base.py:977: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC training time: 2.92 s\n",
      "RF training time: 7.33 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classifier</th>\n",
       "      <th>training_time</th>\n",
       "      <th>train_acc</th>\n",
       "      <th>test_acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RF</td>\n",
       "      <td>7.327809</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.9380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LR</td>\n",
       "      <td>0.398792</td>\n",
       "      <td>0.912875</td>\n",
       "      <td>0.9210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SVC</td>\n",
       "      <td>2.917563</td>\n",
       "      <td>0.912750</td>\n",
       "      <td>0.9205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NB</td>\n",
       "      <td>1.731807</td>\n",
       "      <td>0.910375</td>\n",
       "      <td>0.9185</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  classifier  training_time  train_acc  test_acc\n",
       "3         RF       7.327809   1.000000    0.9380\n",
       "1         LR       0.398792   0.912875    0.9210\n",
       "2        SVC       2.917563   0.912750    0.9205\n",
       "0         NB       1.731807   0.910375    0.9185"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "res_df = classify_data(X_train, y_train, X_test, y_test)\n",
    "display(res_df.sort_values(by='test_acc', ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.01934,
     "end_time": "2021-02-14T21:06:58.839456",
     "exception": false,
     "start_time": "2021-02-14T21:06:58.820116",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-14T21:06:58.884620Z",
     "iopub.status.busy": "2021-02-14T21:06:58.883912Z",
     "iopub.status.idle": "2021-02-14T21:06:58.887669Z",
     "shell.execute_reply": "2021-02-14T21:06:58.887072Z"
    },
    "papermill": {
     "duration": 0.028278,
     "end_time": "2021-02-14T21:06:58.887801",
     "exception": false,
     "start_time": "2021-02-14T21:06:58.859523",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('0.23.2', '1.0.0')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn\n",
    "sklearn.__version__, joblib.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-14T21:06:58.936498Z",
     "iopub.status.busy": "2021-02-14T21:06:58.935771Z",
     "iopub.status.idle": "2021-02-14T21:06:58.944793Z",
     "shell.execute_reply": "2021-02-14T21:06:58.944222Z"
    },
    "papermill": {
     "duration": 0.036398,
     "end_time": "2021-02-14T21:06:58.944927",
     "exception": false,
     "start_time": "2021-02-14T21:06:58.908529",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['RF.joblib', 'SVC.joblib', 'LR.joblib', 'NB.joblib']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saved_models = [pt for pt in os.listdir('.') if pt.endswith('joblib')]\n",
    "saved_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-14T21:06:59.010007Z",
     "iopub.status.busy": "2021-02-14T21:06:59.009137Z",
     "iopub.status.idle": "2021-02-14T21:06:59.016428Z",
     "shell.execute_reply": "2021-02-14T21:06:59.016928Z"
    },
    "papermill": {
     "duration": 0.047499,
     "end_time": "2021-02-14T21:06:59.017144",
     "exception": false,
     "start_time": "2021-02-14T21:06:58.969645",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "def load_sklearn_joblib_model(path, n_jobs = 1):\n",
    "    # Load from file\n",
    "    model = joblib.load(path)\n",
    "\n",
    "    # make n_jobs = 1, to avoid oversubcription.\n",
    "    # because esemble models like RF,GBM are already parallelized even for predict method.\n",
    "    # ensemble models build the trees parallely using all cores.\n",
    "    if 'n_jobs' in model.get_params().keys():\n",
    "        n_jobs_param = {'n_jobs': n_jobs}\n",
    "        model.set_params(**n_jobs_param)\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.030676,
     "end_time": "2021-02-14T21:06:59.073491",
     "exception": false,
     "start_time": "2021-02-14T21:06:59.042815",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Will Parallelizing the predict method (n_jobs > 1) for sklearn improve inference time ?\n",
    "\n",
    "- We can use Joblib to parallelize the predict method of an algo if it is not already.\n",
    "- If it is already parallelized, we can use n_jobs parameter.\n",
    "\n",
    "### How to Tell if an algo used sklearn is already parallelized ?\n",
    "\n",
    "- A Hack: using CPUs/second. What that means ?\n",
    "\n",
    "If you ever used %%time magic command, it will give two outputs\n",
    "\n",
    "`CPU times: user 42.2 ms, sys: 6.23 ms, total: 48.4 ms\n",
    " Wall time: 47 ms`\n",
    " \n",
    " CPU times: Total time your cpus took (all cores).\n",
    " Wall time: Time took to run the function (Wait time to get the result).\n",
    " \n",
    " `CPU/second = CPU times / Wall time`\n",
    " \n",
    " - if CPU/s =~ 1 , it means the algo is running only on single core using it 100%.\n",
    " - if CPU/s >> 1 (significantly), multiple cores are being used i.e. parallelized.\n",
    " \n",
    " - if CPU/s << 1: The lower the number, the more of its time the process spent waiting (for the network, or the harddrive, or locks, or other processes to release the CPU, or just sleeping). E.g. if CPU/s is 0.75, 25% of the time was spent waiting.\n",
    "\n",
    "To explore more on this:\n",
    "\n",
    "https://pythonspeed.com/articles/blocking-cpu-or-io/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-14T21:06:59.127376Z",
     "iopub.status.busy": "2021-02-14T21:06:59.126500Z",
     "iopub.status.idle": "2021-02-14T21:06:59.129812Z",
     "shell.execute_reply": "2021-02-14T21:06:59.128996Z"
    },
    "papermill": {
     "duration": 0.032914,
     "end_time": "2021-02-14T21:06:59.129982",
     "exception": false,
     "start_time": "2021-02-14T21:06:59.097068",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def timer(f):\n",
    "    \"\"\"\n",
    "    A simple python decorator\n",
    "    to calculate CPU & Wall times\n",
    "    of any function\n",
    "    \"\"\"\n",
    "    \n",
    "    import time\n",
    "    def timed(*args, **kw):\n",
    "        cs, ws = time.process_time(), time.time()\n",
    "        result = f(*args, **kw)\n",
    "        ce, we = time.process_time(), time.time()\n",
    "        ct, wt = ce-cs, we-ws\n",
    "        print(f\"func: {f.__name__}, CPU/s: {ct/wt:.4f}\")\n",
    "        print(f\"CPUtimes: {ct:.4f} s , Walltime: {wt:.4f} s\") \n",
    "        return result\n",
    "    return timed\n",
    "\n",
    "\n",
    "@timer\n",
    "def joblib_predict(model, X):\n",
    "    predictions = model.predict(X)\n",
    "    return predictions.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-14T21:06:59.201586Z",
     "iopub.status.busy": "2021-02-14T21:06:59.200782Z",
     "iopub.status.idle": "2021-02-14T21:07:51.585207Z",
     "shell.execute_reply": "2021-02-14T21:07:51.584468Z"
    },
    "papermill": {
     "duration": 52.422765,
     "end_time": "2021-02-14T21:07:51.585389",
     "exception": false,
     "start_time": "2021-02-14T21:06:59.162624",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: (20000, 10)\n",
      "\n",
      "Model:  RF.joblib\n",
      "func: joblib_predict, CPU/s: 0.9998\n",
      "CPUtimes: 0.2574 s , Walltime: 0.2574 s\n",
      "\n",
      "Model:  SVC.joblib\n",
      "func: joblib_predict, CPU/s: 1.9948\n",
      "CPUtimes: 0.0143 s , Walltime: 0.0071 s\n",
      "\n",
      "Model:  LR.joblib\n",
      "func: joblib_predict, CPU/s: 2.7214\n",
      "CPUtimes: 0.0047 s , Walltime: 0.0017 s\n",
      "\n",
      "Model:  NB.joblib\n",
      "func: joblib_predict, CPU/s: 1.0004\n",
      "CPUtimes: 0.0043 s , Walltime: 0.0043 s\n"
     ]
    }
   ],
   "source": [
    "# generate fake data for inference\n",
    "X_infer, y_infer = datasets.make_classification(n_samples=20000, n_features=10)\n",
    "print('Input:', X_infer.shape)\n",
    "\n",
    "\n",
    "for load_path in saved_models:\n",
    "    print('\\nModel: ',load_path)\n",
    "    clf = load_sklearn_joblib_model(load_path)\n",
    "    time.sleep(3)\n",
    "    preds = joblib_predict(clf, X_infer)\n",
    "    time.sleep(10)\n",
    "    del clf, preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.037333,
     "end_time": "2021-02-14T21:07:51.660635",
     "exception": false,
     "start_time": "2021-02-14T21:07:51.623302",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### From the above results:\n",
    " - LR - predict is parallelized.\n",
    " - GaussianNB - No n_jobs & predict method is not parallelized.\n",
    " - LinearSVC - No n_jobs & predict method runs in parallel.\n",
    " - RandomForest - parallelized, but can be controlled using n_jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-14T21:07:51.744091Z",
     "iopub.status.busy": "2021-02-14T21:07:51.743178Z",
     "iopub.status.idle": "2021-02-14T21:07:51.750029Z",
     "shell.execute_reply": "2021-02-14T21:07:51.750605Z"
    },
    "papermill": {
     "duration": 0.053207,
     "end_time": "2021-02-14T21:07:51.750802",
     "exception": false,
     "start_time": "2021-02-14T21:07:51.697595",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of cpus: 4\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import gen_batches\n",
    "from joblib import Parallel, delayed, cpu_count\n",
    "print('num of cpus:', cpu_count())\n",
    "\n",
    "@timer\n",
    "def joblib_parallel_predict(model, X, \n",
    "                            jb_kwargs = {'prefer': \"threads\",\n",
    "                                         'require': \"sharedmem\"}):\n",
    "    n_jobs = max(cpu_count(), 1)\n",
    "    slices = gen_batches(len(X), len(X)//n_jobs)\n",
    "    parallel = Parallel(n_jobs=n_jobs, **jb_kwargs)\n",
    "    results = parallel(delayed(model.predict)(X[s]) for s in slices)\n",
    "    return np.vstack(results).flatten().tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.025271,
     "end_time": "2021-02-14T21:07:51.801921",
     "exception": false,
     "start_time": "2021-02-14T21:07:51.776650",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "- Since, The predict method of NB is not parallelized, I will try to use Joblib's Parallel & delayed functions to make it Parallel across all the cores.\n",
    "\n",
    "- What happens if try to parallelize the code which is already runs in all cores ? It results in oversubcriptions of threads, increasing overhead. So the function runs even slower."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-14T21:07:51.857825Z",
     "iopub.status.busy": "2021-02-14T21:07:51.857224Z",
     "iopub.status.idle": "2021-02-14T21:07:54.975733Z",
     "shell.execute_reply": "2021-02-14T21:07:54.974618Z"
    },
    "papermill": {
     "duration": 3.149373,
     "end_time": "2021-02-14T21:07:54.975897",
     "exception": false,
     "start_time": "2021-02-14T21:07:51.826524",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model:  NB.joblib\n",
      "func: joblib_parallel_predict, CPU/s: 0.1467\n",
      "CPUtimes: 0.0157 s , Walltime: 0.1071 s\n"
     ]
    }
   ],
   "source": [
    "load_path = 'NB.joblib'\n",
    "print('\\nModel: ', load_path)\n",
    "clf = load_sklearn_joblib_model(load_path)\n",
    "time.sleep(3)\n",
    "preds = joblib_parallel_predict(clf, X_infer)\n",
    "assert len(preds) == len(X_infer)\n",
    "del clf, preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.025015,
     "end_time": "2021-02-14T21:07:55.027460",
     "exception": false,
     "start_time": "2021-02-14T21:07:55.002445",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "- One thing to notice: CPU/s is increased only slightly (i.e only using all cores partially), Parallelized NB still takes way more time than original one (0.10 > 0.0047) for batch size of 20k. \n",
    "\n",
    "- This is due to the overhead of slicing data & accumulating the results. Also, sklearn models are mostly optimized for larger batches. So let us check at what is minimum batch size to use to leverage power of parallelzing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-14T21:07:55.089779Z",
     "iopub.status.busy": "2021-02-14T21:07:55.088887Z",
     "iopub.status.idle": "2021-02-14T21:13:35.050141Z",
     "shell.execute_reply": "2021-02-14T21:13:35.051111Z"
    },
    "papermill": {
     "duration": 339.99797,
     "end_time": "2021-02-14T21:13:35.051326",
     "exception": false,
     "start_time": "2021-02-14T21:07:55.053356",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:  NB.joblib\n",
      "\n",
      "Input: (10, 10) Parallel:  False\n",
      "func: joblib_predict, CPU/s: 1.0362\n",
      "CPUtimes: 0.0004 s , Walltime: 0.0004 s\n",
      "\n",
      "Input: (10, 10) Parallel:  threads\n",
      "func: joblib_parallel_predict, CPU/s: 0.0757\n",
      "CPUtimes: 0.0078 s , Walltime: 0.1034 s\n",
      "\n",
      "Input: (10, 10) Parallel:  processes\n",
      "func: joblib_parallel_predict, CPU/s: 0.8672\n",
      "CPUtimes: 0.0135 s , Walltime: 0.0156 s\n",
      "--------------------------------------------------\n",
      "Model:  NB.joblib\n",
      "\n",
      "Input: (100, 10) Parallel:  False\n",
      "func: joblib_predict, CPU/s: 1.0074\n",
      "CPUtimes: 0.0006 s , Walltime: 0.0006 s\n",
      "\n",
      "Input: (100, 10) Parallel:  threads\n",
      "func: joblib_parallel_predict, CPU/s: 0.0625\n",
      "CPUtimes: 0.0065 s , Walltime: 0.1035 s\n",
      "\n",
      "Input: (100, 10) Parallel:  processes\n",
      "func: joblib_parallel_predict, CPU/s: 0.9156\n",
      "CPUtimes: 0.0093 s , Walltime: 0.0101 s\n",
      "--------------------------------------------------\n",
      "Model:  NB.joblib\n",
      "\n",
      "Input: (1000, 10) Parallel:  False\n",
      "func: joblib_predict, CPU/s: 1.0058\n",
      "CPUtimes: 0.0006 s , Walltime: 0.0006 s\n",
      "\n",
      "Input: (1000, 10) Parallel:  threads\n",
      "func: joblib_parallel_predict, CPU/s: 0.1046\n",
      "CPUtimes: 0.0108 s , Walltime: 0.1034 s\n",
      "\n",
      "Input: (1000, 10) Parallel:  processes\n",
      "func: joblib_parallel_predict, CPU/s: 0.9527\n",
      "CPUtimes: 0.0134 s , Walltime: 0.0141 s\n",
      "--------------------------------------------------\n",
      "Model:  NB.joblib\n",
      "\n",
      "Input: (10000, 10) Parallel:  False\n",
      "func: joblib_predict, CPU/s: 1.0012\n",
      "CPUtimes: 0.0024 s , Walltime: 0.0024 s\n",
      "\n",
      "Input: (10000, 10) Parallel:  threads\n",
      "func: joblib_parallel_predict, CPU/s: 0.1427\n",
      "CPUtimes: 0.0147 s , Walltime: 0.1028 s\n",
      "\n",
      "Input: (10000, 10) Parallel:  processes\n",
      "func: joblib_parallel_predict, CPU/s: 0.9013\n",
      "CPUtimes: 0.0129 s , Walltime: 0.0143 s\n",
      "--------------------------------------------------\n",
      "Model:  NB.joblib\n",
      "\n",
      "Input: (100000, 10) Parallel:  False\n",
      "func: joblib_predict, CPU/s: 0.9906\n",
      "CPUtimes: 0.0438 s , Walltime: 0.0442 s\n",
      "\n",
      "Input: (100000, 10) Parallel:  threads\n",
      "func: joblib_parallel_predict, CPU/s: 0.4356\n",
      "CPUtimes: 0.0454 s , Walltime: 0.1043 s\n",
      "\n",
      "Input: (100000, 10) Parallel:  processes\n",
      "func: joblib_parallel_predict, CPU/s: 0.1493\n",
      "CPUtimes: 0.0203 s , Walltime: 0.1359 s\n",
      "--------------------------------------------------\n",
      "Model:  NB.joblib\n",
      "\n",
      "Input: (1000000, 10) Parallel:  False\n",
      "func: joblib_predict, CPU/s: 0.9995\n",
      "CPUtimes: 0.3963 s , Walltime: 0.3965 s\n",
      "\n",
      "Input: (1000000, 10) Parallel:  threads\n",
      "func: joblib_parallel_predict, CPU/s: 1.5257\n",
      "CPUtimes: 0.3618 s , Walltime: 0.2371 s\n",
      "\n",
      "Input: (1000000, 10) Parallel:  processes\n",
      "func: joblib_parallel_predict, CPU/s: 0.3004\n",
      "CPUtimes: 0.0982 s , Walltime: 0.3269 s\n",
      "--------------------------------------------------\n",
      "Model:  NB.joblib\n",
      "\n",
      "Input: (10000000, 10) Parallel:  False\n",
      "func: joblib_predict, CPU/s: 0.9996\n",
      "CPUtimes: 2.2422 s , Walltime: 2.2430 s\n",
      "\n",
      "Input: (10000000, 10) Parallel:  threads\n",
      "func: joblib_parallel_predict, CPU/s: 2.3908\n",
      "CPUtimes: 3.4988 s , Walltime: 1.4634 s\n",
      "\n",
      "Input: (10000000, 10) Parallel:  processes\n",
      "func: joblib_parallel_predict, CPU/s: 0.3858\n",
      "CPUtimes: 0.6902 s , Walltime: 1.7892 s\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for ns in [10, 100, 10**3, 10**4, 10**5, 10**6, 10**7]:\n",
    "    load_path = 'NB.joblib'\n",
    "    print('Model: ', load_path)\n",
    "    X_infer, _ = datasets.make_classification(n_samples=ns, n_features=10)\n",
    "    \n",
    "    # direct predict\n",
    "    print('\\nInput:', X_infer.shape, 'Parallel: ', False)\n",
    "    clf = load_sklearn_joblib_model(load_path)\n",
    "    time.sleep(5)\n",
    "    preds = joblib_predict(clf, X_infer)\n",
    "    time.sleep(10)\n",
    "    del clf, preds\n",
    "    \n",
    "    # parallel predict threads\n",
    "    print('\\nInput:', X_infer.shape, 'Parallel: ', 'threads')\n",
    "    clf = load_sklearn_joblib_model(load_path)\n",
    "    time.sleep(5)\n",
    "    preds = joblib_parallel_predict(clf, X_infer)\n",
    "    assert len(preds) == len(X_infer)\n",
    "    time.sleep(10)\n",
    "    del clf, preds\n",
    "    \n",
    "    # parallel predict processes\n",
    "    print('\\nInput:', X_infer.shape, 'Parallel: ', 'processes')\n",
    "    clf = load_sklearn_joblib_model(load_path)\n",
    "    time.sleep(5)\n",
    "    preds = joblib_parallel_predict(clf, X_infer, jb_kwargs = {'prefer': 'processes'})\n",
    "    assert len(preds) == len(X_infer)\n",
    "    print('-'*50)\n",
    "    time.sleep(10)\n",
    "    del clf, preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.061069,
     "end_time": "2021-02-14T21:13:35.176841",
     "exception": false,
     "start_time": "2021-02-14T21:13:35.115772",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "- only at batch size >= 10**6, the parallelizing is showing some use\n",
    "- But one important thing is CPU/s is << 1 for lower/mini batches i.e indicating that CPU is waiting most of the time, can we levarge that somehow ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-14T21:13:35.305284Z",
     "iopub.status.busy": "2021-02-14T21:13:35.304444Z",
     "iopub.status.idle": "2021-02-14T21:13:35.444185Z",
     "shell.execute_reply": "2021-02-14T21:13:35.444701Z"
    },
    "papermill": {
     "duration": 0.206013,
     "end_time": "2021-02-14T21:13:35.444872",
     "exception": false,
     "start_time": "2021-02-14T21:13:35.238859",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 10)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_infer, _ = datasets.make_classification(n_samples=100000, n_features=10)\n",
    "X_infer.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-14T21:13:35.531084Z",
     "iopub.status.busy": "2021-02-14T21:13:35.530392Z",
     "iopub.status.idle": "2021-02-14T21:13:35.555559Z",
     "shell.execute_reply": "2021-02-14T21:13:35.554598Z"
    },
    "papermill": {
     "duration": 0.070737,
     "end_time": "2021-02-14T21:13:35.555765",
     "exception": false,
     "start_time": "2021-02-14T21:13:35.485028",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "func: joblib_predict, CPU/s: 1.0338\n",
      "CPUtimes: 0.0202 s , Walltime: 0.0195 s\n"
     ]
    }
   ],
   "source": [
    "clf = load_sklearn_joblib_model(load_path)\n",
    "r = joblib_predict(model = clf, X = X_infer)\n",
    "# print(len(r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-14T21:13:35.685082Z",
     "iopub.status.busy": "2021-02-14T21:13:35.684221Z",
     "iopub.status.idle": "2021-02-14T21:13:35.689333Z",
     "shell.execute_reply": "2021-02-14T21:13:35.690004Z"
    },
    "papermill": {
     "duration": 0.071632,
     "end_time": "2021-02-14T21:13:35.690255",
     "exception": false,
     "start_time": "2021-02-14T21:13:35.618623",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from functools import partial\n",
    "import asyncio\n",
    "\n",
    "def blocking_io():\n",
    "    # File operations (such as logging) can block the\n",
    "    # event loop: run them in a thread pool.\n",
    "    with open('/dev/urandom', 'rb') as f:\n",
    "        return f.read(100)\n",
    "\n",
    "\n",
    "async def joblib_predict_async(**kwargs):\n",
    "    loop = asyncio.get_running_loop()\n",
    "    # func_args = {'model': clf, 'X': X_infer}\n",
    "    with ThreadPoolExecutor() as pool:\n",
    "        result = await loop.run_in_executor(\n",
    "            pool, partial(joblib_predict, **kwargs))\n",
    "        # print('thread pool', len(result))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-14T21:13:35.825286Z",
     "iopub.status.busy": "2021-02-14T21:13:35.824401Z",
     "iopub.status.idle": "2021-02-14T21:13:35.854557Z",
     "shell.execute_reply": "2021-02-14T21:13:35.852241Z"
    },
    "papermill": {
     "duration": 0.101189,
     "end_time": "2021-02-14T21:13:35.854725",
     "exception": false,
     "start_time": "2021-02-14T21:13:35.753536",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "func: joblib_predict, CPU/s: 1.0408\n",
      "CPUtimes: 0.0227 s , Walltime: 0.0218 s\n"
     ]
    }
   ],
   "source": [
    "clf = load_sklearn_joblib_model(load_path)\n",
    "r = await joblib_predict_async(model = clf, X = X_infer)\n",
    "# print(len(r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-14T21:13:35.946562Z",
     "iopub.status.busy": "2021-02-14T21:13:35.945899Z",
     "iopub.status.idle": "2021-02-14T21:13:35.949689Z",
     "shell.execute_reply": "2021-02-14T21:13:35.948833Z"
    },
    "papermill": {
     "duration": 0.054087,
     "end_time": "2021-02-14T21:13:35.949840",
     "exception": false,
     "start_time": "2021-02-14T21:13:35.895753",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "@timer\n",
    "async def predict_parallel_async(model, X):\n",
    "    loop = asyncio.get_event_loop()\n",
    "    n_jobs = max(cpu_count(), 1)\n",
    "    slices = gen_batches(len(X), len(X)//n_jobs)\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=n_jobs) as pool:\n",
    "        tasks = [loop.run_in_executor(pool, model.predict, X[s]) for s in slices]\n",
    "\n",
    "    completed, pending = await asyncio.wait(tasks)\n",
    "    results = []\n",
    "    for t in completed:\n",
    "        results.extend(t.result().tolist())\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-14T21:13:36.042411Z",
     "iopub.status.busy": "2021-02-14T21:13:36.041751Z",
     "iopub.status.idle": "2021-02-14T21:13:36.068785Z",
     "shell.execute_reply": "2021-02-14T21:13:36.064444Z"
    },
    "papermill": {
     "duration": 0.076639,
     "end_time": "2021-02-14T21:13:36.068929",
     "exception": false,
     "start_time": "2021-02-14T21:13:35.992290",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "func: predict_parallel_async, CPU/s: 2.2230\n",
      "CPUtimes: 0.0000 s , Walltime: 0.0000 s\n",
      "0.02070903778076172\n",
      "100000\n"
     ]
    }
   ],
   "source": [
    "s= time.time()\n",
    "clf = load_sklearn_joblib_model(load_path)\n",
    "r = await predict_parallel_async(model = clf, X = X_infer)\n",
    "print(time.time() - s)\n",
    "print(len(r))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.04113,
     "end_time": "2021-02-14T21:13:36.151819",
     "exception": false,
     "start_time": "2021-02-14T21:13:36.110689",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "- Asyncio also didn't help to improve inference times at smaller batches (expected as model.predict is cpu bound.)\n",
    "\n",
    "- Joblib natively wont support async/await.\n",
    "\n",
    "### WHY PARALLELIZING DIDN'T HELP ?\n",
    "\n",
    "#### Training:\n",
    "\n",
    "1. Both Thread Based & Process Based Parallelism can help. Why ?\n",
    "\n",
    "- Because Training is a iterative process, the observarions must be used for train the algo multiple times. So in this case, With little overhead of creating process & sharing data the training can be faster.\n",
    "\n",
    "- Also Training is done on large amount of data, so large batch sizes helps. (sklearn & numpy are optimized for matrix/larger batch calculations)\n",
    "\n",
    "#### Predictions:\n",
    "\n",
    "- Prediction is a single step process (we only use observation once to get prediction), So using process based parallelism adds unneccessary overhead and will not help in most cases.\n",
    "\n",
    "\n",
    "- Usually predictions/inference happens on smaller set of data. Thread based parallelism can help to speed up the algo but if inference data is small then overhead of creating threads can overtake normal execution time. so If inference is running on smaller batches/single observation it may not improve the results.\n",
    "\n",
    "\n",
    "- https://github.com/scikit-learn/scikit-learn/issues/7448\n",
    "- https://github.com/scikit-learn/scikit-learn/pull/16310\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.041543,
     "end_time": "2021-02-14T21:13:36.235429",
     "exception": false,
     "start_time": "2021-02-14T21:13:36.193886",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Solution to Faster & Scalable Machine Learning Inference APIs:\n",
    "\n",
    "This can be tricky to answer without excat suitation but here are some options to try.\n",
    "\n",
    "1. Make code/algo simpler so that it can run the predictions for single/mini-batch observation(s) in minimal possible time. Further, combine this ASGI server (reduces I/O time) with multiple workers for better request latency.\n",
    "\n",
    "some framework(s) doing this: \n",
    "1. onnx (supports sklearn, xgboost & lgbm)\n",
    "2. river\n",
    "\n",
    "https://cloudblogs.microsoft.com/opensource/2020/12/17/accelerate-simplify-scikit-learn-model-inference-onnx-runtime/\n",
    "\n",
    "\n",
    "OR\n",
    "\n",
    "2. Use a Queue to collect the request, make predictions in batches & return response for all at once. Further, this can be also combined with ASGI server for better latency.\n",
    "\n",
    "some framework(s) doing this: \n",
    "\n",
    "1. tf-serving (tensorflow)\n",
    "2. BentoML (all major ml frameworks)\n",
    "3. Clipper\n",
    "\n",
    "OR\n",
    "\n",
    "3.Other Alternative(s): \n",
    "\n",
    "- Using Distrubuted systems (Spark, Dask)\n",
    "- Humming Bird (https://github.com/microsoft/hummingbird)\n",
    "- Processor specific accelrators (like Intel's OpenVINO)\n",
    "- Using GPU based infernece (TensorRT)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 418.763677,
   "end_time": "2021-02-14T21:13:37.089547",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-02-14T21:06:38.325870",
   "version": "2.2.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
